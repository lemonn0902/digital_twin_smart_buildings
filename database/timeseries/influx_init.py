#!/usr/bin/env python3
"""
Seed InfluxDB with ASHRAE deployment data and simulate IoT streams.

This script reads the processed dataset generated by `scripts/fetch_dataset.py`
and writes the metrics into InfluxDB so that the backend can query real data.

Usage examples:

    # Seed entire history into InfluxDB
    python database/timeseries/influx_init.py --mode seed

    # Stream data in "real time" (1 record/second) to mimic IoT sensors
    python database/timeseries/influx_init.py --mode stream --speed 1.0
"""

import argparse
import json
import os
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Iterable, List, Optional

import numpy as np
import pandas as pd
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    # Load .env from project root (3 levels up from this script)
    PROJECT_ROOT = Path(__file__).resolve().parents[2]
    env_path = PROJECT_ROOT / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        print(f"Loaded environment variables from {env_path}")
    else:
        print(f"Warning: .env file not found at {env_path}")
except ImportError:
    print("Warning: python-dotenv not installed. Environment variables must be set manually.")
    PROJECT_ROOT = Path(__file__).resolve().parents[2]

# Workspace paths (PROJECT_ROOT is set above)
DATA_DIR = PROJECT_ROOT / "data" / "processed"
DEPLOYMENT_DATA = DATA_DIR / "deployment_data.csv"
BUILDING_INFO = DATA_DIR / "building_info.json"

# Simulated zone configuration to create richer telemetry
ZONE_PROFILES: Dict[str, Dict[str, float]] = {
    "zone-east": {"energy_ratio": 0.4, "temp_bias": 0.6, "occ_bias": 0.05},
    "zone-west": {"energy_ratio": 0.35, "temp_bias": -0.4, "occ_bias": -0.02},
    "zone-core": {"energy_ratio": 0.25, "temp_bias": 0.0, "occ_bias": 0.01},
}


def trim_leading_zero_energy_rows(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    if "energy" not in df.columns:
        return df

    energy = pd.to_numeric(df["energy"], errors="coerce").fillna(0.0)
    non_zero_idx = energy.gt(0.0)
    if not bool(non_zero_idx.any()):
        return df
    first_valid = int(non_zero_idx.idxmax())
    return df.loc[first_valid:].reset_index(drop=True)


def require_dataset() -> pd.DataFrame:
    """Ensure processed dataset exists and load it."""
    if not DEPLOYMENT_DATA.exists():
        raise FileNotFoundError(
            "deployment_data.csv not found. "
            "Run `python scripts/fetch_dataset.py` first."
        )

    df = pd.read_csv(DEPLOYMENT_DATA, parse_dates=["timestamp"])
    if df.empty:
        raise ValueError("deployment_data.csv is empty.")

    return df.sort_values("timestamp").reset_index(drop=True)


def load_building_info() -> Dict:
    """Load metadata about the selected building."""
    # Always use "demo-building" as the building_id for consistency with application queries
    # The application expects "demo-building" in all queries
    if BUILDING_INFO.exists():
        with open(BUILDING_INFO) as fp:
            info = json.load(fp)
            # Override building_id to always use "demo-building"
            info["building_id"] = "demo-building"
            return info

    # Fallback metadata
    return {
        "building_id": "demo-building",
        "primary_use": "Office",
        "square_feet": 50000,
        "site_id": 0,
    }


def get_influx_client():
    """Create InfluxDB client and ensure the bucket exists."""
    influx_url = os.getenv("INFLUXDB_URL", "http://localhost:8086")
    influx_token = os.getenv("INFLUXDB_TOKEN")
    influx_org = os.getenv("INFLUXDB_ORG", "digital-twin")
    influx_bucket = os.getenv("INFLUXDB_BUCKET", "building_telemetry")

    if not influx_token:
        raise EnvironmentError(
            "INFLUXDB_TOKEN is not set. "
            "Create an API token in InfluxDB and export it before running."
        )

    # For macOS SSL issues, allow disabling verification (testing only)
    verify_ssl = os.getenv("INFLUXDB_VERIFY_SSL", "true").lower() != "false"
    client = InfluxDBClient(url=influx_url, token=influx_token, org=influx_org, verify_ssl=verify_ssl)
    buckets_api = client.buckets_api()
    bucket = buckets_api.find_bucket_by_name(influx_bucket)
    if bucket is None:
        org = client.organizations_api().find_organization(influx_org)
        buckets_api.create_bucket(bucket_name=influx_bucket, org_id=org.id)
        print(f"Created bucket '{influx_bucket}' in org '{influx_org}'.")

    return client, influx_bucket, influx_org


def build_points(row: pd.Series, building_id: str, timestamp_offset: Optional[timedelta] = None) -> List[Point]:
    """Convert one timestamp row into multiple zone metrics.
    
    Args:
        row: DataFrame row with timestamp and metrics
        building_id: Building identifier
        timestamp_offset: Optional offset to apply to timestamps (for mapping old dates to recent dates)
    """
    original_timestamp = row["timestamp"].to_pydatetime()
    
    # Apply offset if provided (to map old timestamps to recent dates)
    if timestamp_offset:
        timestamp = original_timestamp + timestamp_offset
    else:
        timestamp = original_timestamp
    
    try:
        energy = float(row.get("energy", 0.0))
    except Exception:
        energy = 0.0
    if pd.isna(energy) or energy <= 0.0:
        hour = int(timestamp.hour)
        dayofweek = int(timestamp.weekday())
        is_workday = 1.0 if dayofweek < 5 else 0.0
        is_work_hours = 1.0 if 7 <= hour <= 19 else 0.0

        occupancy_factor = float(np.clip(row.get("occupancy", 0.3), 0.0, 1.0))
        temp_val = row.get("temperature")
        try:
            temp_float = float(temp_val) if not pd.isna(temp_val) else 22.0
        except Exception:
            temp_float = 22.0

        base = 40.0 + 60.0 * is_workday * is_work_hours
        occ_load = 40.0 * occupancy_factor
        temp_load = max(0.0, abs(temp_float - 22.0)) * 3.0

        seed = int(timestamp.strftime("%Y%m%d%H"))
        noise = float(np.random.default_rng(seed).normal(0.0, 4.0))
        energy = max(5.0, base + occ_load + temp_load + noise)
    temperature = row.get("temperature")
    humidity = row.get("humidity")
    occupancy = np.clip(row.get("occupancy", 0.3), 0.0, 1.0)

    points: List[Point] = []
    for zone, profile in ZONE_PROFILES.items():
        e_val = energy * profile["energy_ratio"]
        t_val = None if pd.isna(temperature) else temperature + profile["temp_bias"]
        h_val = None if pd.isna(humidity) else humidity
        o_val = np.clip(occupancy + profile["occ_bias"], 0.0, 1.0)

        points.append(
            Point("energy")
            .tag("building_id", building_id)
            .tag("zone_id", zone)
            .field("value", float(e_val))
            .time(timestamp, WritePrecision.NS)
        )

        if t_val is not None:
            points.append(
                Point("temperature")
                .tag("building_id", building_id)
                .tag("zone_id", zone)
                .field("value", float(t_val))
                .time(timestamp, WritePrecision.NS)
            )

        if h_val is not None:
            points.append(
                Point("humidity")
                .tag("building_id", building_id)
                .tag("zone_id", zone)
                .field("value", float(h_val))
                .time(timestamp, WritePrecision.NS)
            )

        points.append(
            Point("occupancy")
            .tag("building_id", building_id)
            .tag("zone_id", zone)
            .field("value", float(o_val))
            .time(timestamp, WritePrecision.NS)
        )

    return points


def seed_historical_data(df: pd.DataFrame, write_api, bucket: str, org: str, building_id: str, limit: int | None = None, use_recent_timestamps: bool = True) -> None:
    """Bulk insert historical data into InfluxDB.
    
    Args:
        df: DataFrame with historical data
        write_api: InfluxDB write API
        bucket: InfluxDB bucket name
        org: InfluxDB organization
        building_id: Building identifier
        limit: Optional limit on number of rows to seed
        use_recent_timestamps: If True, map old timestamps to recent dates (default: True)
    """
    total_rows = len(df) if limit is None else min(limit, len(df))
    
    # Calculate timestamp offset to map old dates to recent dates
    timestamp_offset = None
    if use_recent_timestamps and not df.empty:
        # Get the first timestamp from the dataset (likely 2016)
        first_timestamp = df["timestamp"].iloc[0].to_pydatetime()
        
        # Map to start 29 days ago (within 30-day retention period, leaves 1 day buffer)
        target_start = datetime.now() - timedelta(days=29)
        
        # Calculate offset
        timestamp_offset = target_start - first_timestamp
        
        print(f"Mapping timestamps from {first_timestamp.date()} to recent dates (starting {target_start.date()})...")
        print(f"Timestamp offset: {timestamp_offset.days} days")
    
    print(f"Seeding {total_rows} rows into InfluxDB...")

    batch: List[Point] = []
    batch_size = 500  # Reduced from 5000 to avoid rate limits on free tier

    for idx, row in df.head(total_rows).iterrows():
        batch.extend(build_points(row, building_id, timestamp_offset))
        if len(batch) >= batch_size:
            write_api.write(bucket=bucket, org=org, record=batch)
            batch = []
            if idx % 100 == 0:
                print(f"  wrote {idx + 1} rows...")
            # Add delay to avoid rate limits on free tier
            time.sleep(0.5)

    if batch:
        write_api.write(bucket=bucket, org=org, record=batch)

    print("Historical seed complete.")
    if timestamp_offset:
        last_timestamp = df["timestamp"].iloc[min(total_rows - 1, len(df) - 1)].to_pydatetime() + timestamp_offset
        print(f"Data range: {target_start.date()} to {last_timestamp.date()}")
        print("âœ… Data is now queryable by 'last 30 days' and 'latest metrics' endpoints!")


def stream_data(df: pd.DataFrame, write_api, bucket: str, org: str, building_id: str, speed: float, loop: bool) -> None:
    """Continuously stream data points to InfluxDB to mimic IoT telemetry.
    
    Uses current timestamps (datetime.now()) for real-time simulation.
    """
    print(f"Streaming data at {speed}s intervals (loop={loop})...")
    print("Using current timestamps for real-time simulation...")
    
    while True:
        for row in df.itertuples(index=False):
            # For streaming, calculate offset to map CSV timestamp to current time
            row_dict = row._asdict()
            row_series = pd.Series(row_dict)
            original_timestamp = row_series["timestamp"].to_pydatetime()
            # Offset to map original timestamp to now
            current_time_offset = datetime.now() - original_timestamp
            points = build_points(row_series, building_id, timestamp_offset=current_time_offset)
            write_api.write(bucket=bucket, org=org, record=points)
            time.sleep(max(speed, 0.01))
        if not loop:
            break
    print("Streaming finished.")


def parse_args():
    parser = argparse.ArgumentParser(description="Seed InfluxDB with building telemetry.")
    parser.add_argument(
        "--mode",
        choices=["seed", "stream"],
        default="seed",
        help="Seed entire dataset or simulate live streaming.",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Limit number of rows when seeding (for quick tests).",
    )
    parser.add_argument(
        "--speed",
        type=float,
        default=1.0,
        help="Seconds between points when streaming.",
    )
    parser.add_argument(
        "--loop",
        action="store_true",
        help="When streaming, loop over the dataset indefinitely.",
    )
    parser.add_argument(
        "--use-original-timestamps",
        action="store_true",
        help="Use original timestamps from CSV (default: map to recent dates for queryability).",
    )
    return parser.parse_args()


def main():
    args = parse_args()
    df = require_dataset()
    df = trim_leading_zero_energy_rows(df)
    building_info = load_building_info()
    # Always use "demo-building" to match application queries
    # The application expects "demo-building" in all API endpoints
    building_id = "demo-building"
    print(f"Using building_id: {building_id} (overriding value from building_info.json)")

    client, bucket, org = get_influx_client()
    write_api = client.write_api(write_options=SYNCHRONOUS)

    if args.mode == "seed":
        # By default, use recent timestamps (map old dates to recent dates)
        # This ensures data is queryable by "last 30 days" endpoints
        use_recent = not args.use_original_timestamps
        seed_historical_data(df, write_api, bucket, org, building_id, limit=args.limit, use_recent_timestamps=use_recent)
    else:
        # Streaming always uses current timestamps
        stream_data(df, write_api, bucket, org, building_id, speed=args.speed, loop=args.loop)

    client.close()


if __name__ == "__main__":
    try:
        main()
    except Exception as exc:
        print(f"[ERROR] {exc}")
        sys.exit(1)
